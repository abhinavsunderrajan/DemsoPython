{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Work on a vanilla example focussing on time series prediction using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhinav.sunderrajan/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from matplotlib import pyplot\n",
    "from math import sqrt\n",
    "from pandas import Series\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month\n",
      "1901-01-01    266.0\n",
      "1901-02-01    145.9\n",
      "1901-03-01    183.1\n",
      "1901-04-01    119.3\n",
      "1901-05-01    180.3\n",
      "Name: Sales of shampoo over a three year period, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#read data and some exploratory plots\n",
    "# load dataset\n",
    "def parser(x):\n",
    "\treturn datetime.strptime('190'+x, '%Y-%m')\n",
    "series = read_csv('datasets/shampoo-sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "print(series.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line plot\n",
    "series.plot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = series.values\n",
    "train, test = X[0:-12], X[-12:]\n",
    "\n",
    "# walk-forward validation\n",
    "history = [x for x in train]\n",
    "\n",
    "predictions = list()\n",
    "for i in range(len(test)):\n",
    "\t# make prediction\n",
    "\tpredictions.append(history[-1])\n",
    "\t# observation\n",
    "\thistory.append(test[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 136.761\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1feda438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# report performance\n",
    "rmse = sqrt(mean_squared_error(test, predictions))\n",
    "print('RMSE: %.3f' % rmse)\n",
    "# line plot of observed vs predicted\n",
    "pyplot.plot(test)\n",
    "pyplot.plot(predictions)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# frame a sequence as a supervised learning problem\n",
    "def timeseries_to_supervised(data, lag=1):\n",
    "\tdf = DataFrame(data)\n",
    "\tcolumns = [df.shift(i) for i in range(1, lag+1)]\n",
    "\tcolumns.append(df)\n",
    "\tdf = concat(columns, axis=1)\n",
    "\tdf.fillna(0, inplace=True)\n",
    "\treturn df\n",
    "\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn Series(diff)\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "\treturn yhat + history[-interval]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "\t# fit scaler\n",
    "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\tscaler = scaler.fit(train)\n",
    "\t# transform train\n",
    "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
    "\ttrain_scaled = scaler.transform(train)\n",
    "\t# transform test\n",
    "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
    "\ttest_scaled = scaler.transform(test)\n",
    "\treturn scaler, train_scaled, test_scaled\n",
    " \n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "\tnew_row = [x for x in X] + [value]\n",
    "\tarray = numpy.array(new_row)\n",
    "\tarray = array.reshape(1, len(array))\n",
    "\tinverted = scaler.inverse_transform(array)\n",
    "\treturn inverted[0, -1]\n",
    " \n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "\tX, y = train[:, 0:-1], train[:, -1]\n",
    "\tX = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\tfor i in range(nb_epoch):\n",
    "\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "\t\tmodel.reset_states()\n",
    "\treturn model\n",
    " \n",
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "\tX = X.reshape(1, 1, len(X))\n",
    "\tyhat = model.predict(X, batch_size=batch_size)\n",
    "\treturn yhat[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0      0\n",
      "0    0.0 -120.1\n",
      "1 -120.1   37.2\n",
      "2   37.2  -63.8\n",
      "3  -63.8   61.0\n",
      "4   61.0  -11.8\n"
     ]
    }
   ],
   "source": [
    "# transform data to be stationary\n",
    "raw_values = series.values\n",
    "diff_values = difference(raw_values, 1)\n",
    "\n",
    " \n",
    "# transform data to be supervised learning\n",
    "supervised = timeseries_to_supervised(diff_values, 1)\n",
    "print(supervised.head())\n",
    "supervised_values = supervised.values\n",
    " \n",
    "# split data into train and test-sets\n",
    "train, test = supervised_values[0:-12], supervised_values[-12:]\n",
    " \n",
    "# transform the scale of the data\n",
    "scaler, train_scaled, test_scaled = scale(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.2468     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.2436     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.2408     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.2381     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.2355     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.2328     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.2302     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.2276     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.2250     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.2225     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.2199     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.2173     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.2147     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.2122     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.2096     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.2070     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.2045     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.2019     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1994     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1969     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1943     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1918     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1893     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1869     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1844     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1820     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1796     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1773     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1750     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1727     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1705     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1683     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1662     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1641     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1621     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1601     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1582     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1564     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1546     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1529     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1512     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1496     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1481     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1466     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1452     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.144 - 0s - loss: 0.1439     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1426     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1413     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1402     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1390     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1379     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1369     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1359     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1349     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1340     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1332     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1323     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1315     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1308     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1300     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1293     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1286     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1279     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1273     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1266     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1260     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1254     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1248     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1243     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1237     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1232     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1226     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1221     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1216     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1211     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1206     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1201     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1196     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1191     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1186     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1182     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1177     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1173     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1169     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1164     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1160     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1156     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1152     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1148     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1144     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1140     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1136     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1132     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1128     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1124     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1120     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1116     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1112     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1108     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1104     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1101     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1098     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1095     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1092     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1089     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1086     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1083     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1080     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1077     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1074     \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s - loss: 0.1072     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1069     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1066     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1063     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1060     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1057     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1054     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1051     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1049     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1046     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1043     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1040     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1038     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1035     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1032     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1029     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1027     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1024     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1021     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1018     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1016     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1013     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1010     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1008     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1005     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1003     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1000     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0997     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0994     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0992     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0989     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0987     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0984     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0981     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0978     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0976     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0973     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0971     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0968     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0965     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0963     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0960     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0958     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0955     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0953     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0950     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0947     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0945     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0942     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0940     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0936     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0934     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0931     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0929     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0926     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0924     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0921     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0919     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0916     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0914     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0911     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0909     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0906     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0904     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0901     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0898     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0896     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0893     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0891     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0887     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0885     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0882     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0880     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0877     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0875     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0872     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0870     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0867     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0865     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0862     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0859     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0857     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0854     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0852     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0849     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0846     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0844     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0841     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0838     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0836     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0833     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0830     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0828     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0825     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0822     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0821     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0821     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0814     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0812     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0814     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0808     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0806     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0803     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0802     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0798     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0798     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0791     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0789     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0792     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0784     \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s - loss: 0.0784     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0780     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0778     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0779     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0771     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0772     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0767     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0767     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0763     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0761     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0757     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0759     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0751     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0752     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0746     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0747     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0743     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0740     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0736     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0739     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0730     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0732     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0727     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0723     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0727     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0716     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0719     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0715     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0712     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0707     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0712     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0700     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0704     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0700     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0696     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0692     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0698     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0683     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0689     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0684     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0681     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0676     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0683     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0666     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0677     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0664     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0666     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0663     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0662     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0650     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0666     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0643     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0653     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0648     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0639     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0643     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0644     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0624     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0666     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0616     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0644     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0625     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0611     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0686     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0622     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0736     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0604     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0612     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0742     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0894     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1482     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0664     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0634     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1427     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0914     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0797     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0782     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1043     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1245     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0667     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1050     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1235     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0698     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0942     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0973     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0710     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0770     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0780     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0686     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0665     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0701     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0655     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0625     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0672     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0629     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0608     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0665     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0615     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0600     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0669     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0596     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0590     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0667     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0584     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0587     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0647     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0583     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0609     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0609     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0585     \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s - loss: 0.0666     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0570     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0571     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0690     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0609     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0668     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0574     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0608     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0901     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0648     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0618     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0685     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0887     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1262     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0561     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0752     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1370     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0575     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0670     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1149     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0572     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0632     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0975     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0569     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0601     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0861     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0568     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0575     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0763     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0571     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0571     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0682     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0580     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0608     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0607     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0590     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0698     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0555     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0565     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0744     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0572     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0572     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0649     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0635     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0772     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0545     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0585     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0908     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0617     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0580     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0693     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0746     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0934     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0545     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0674     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.1105     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0563     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0576     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0939     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0593     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0561     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0716     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0616     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.051 - 0s - loss: 0.0652     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0580     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0601     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0765     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0542     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0558     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0762     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0566     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0560     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0646     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0613     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0711     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0541     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0577     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0826     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0567     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0548     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0691     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0652     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0750     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0540     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0607     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0921     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0563     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0541     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0770     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0641     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0668     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0574     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0654     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0905     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0536     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0569     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0863     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0571     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0547     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0686     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0610     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0664     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0558     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0593     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0781     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0535     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0542     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0739     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0572     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0571     \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s - loss: 0.0599     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0593     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0722     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0530     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0551     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0764     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0563     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0554     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0623     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0603     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0724     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0527     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0554     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0791     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0573     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0559     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0621     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0621     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0768     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0526     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0557     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0825     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0570     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0551     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0647     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0616     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0728     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0530     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0563     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0799     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0553     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0540     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0659     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0593     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0664     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0540     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0567     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0768     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0540     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0534     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0671     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0581     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0635     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0547     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0566     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0754     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0537     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0533     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0667     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0588     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0653     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0538     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0569     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0787     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0542     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0534     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0680     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0592     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0657     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0544     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0570     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0779     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0540     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0535     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0677     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0582     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0641     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0549     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0565     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0752     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0537     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0536     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0669     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0572     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0629     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0551     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0560     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0734     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0536     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0537     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0658     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0569     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0635     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0547     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0555     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0732     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0539     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0542     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0643     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0572     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0658     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0541     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0550     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0736     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0546     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0554     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0625     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0574     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0685     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0534     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0543     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0728     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0553     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0570     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0605     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0572     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0705     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0533     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0539     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0707     \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s - loss: 0.0564     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0604     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0574     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0570     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0747     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0536     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0541     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0706     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0575     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0632     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0571     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0570     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0756     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0540     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0545     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0690     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0572     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0636     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0571     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0565     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0735     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0541     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0548     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0678     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0565     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0627     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0574     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0560     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0713     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0542     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0550     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0667     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0561     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0624     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0574     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0557     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0705     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0544     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0553     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0658     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0561     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0635     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0569     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0556     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0712     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0546     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0559     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0651     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0566     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0660     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0563     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0556     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0727     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0550     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0570     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0653     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0576     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0693     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0557     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0556     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0734     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0555     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0581     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0646     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0577     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0703     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0559     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0558     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0733     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0556     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0582     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0646     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0567     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0670     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0568     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0558     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0703     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0554     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0579     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0649     \n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 0s - loss: 0.0567     - ETA: 0s - loss: 0.0\n",
      "Epoch 1/1\n",
      "14/23 [=================>............] - ETA: 0s - loss: 0.0550"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5b168003e54e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlstm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# forecast the entire training dataset to build up state for forecasting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-20ee5dfeb012>\u001b[0m in \u001b[0;36mfit_lstm\u001b[0;34m(train, batch_size, nb_epoch, neurons)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/abhinav.sunderrajan/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/abhinav.sunderrajan/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/Users/abhinav.sunderrajan/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/abhinav.sunderrajan/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/abhinav.sunderrajan/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/abhinav.sunderrajan/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/abhinav.sunderrajan/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/abhinav.sunderrajan/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/abhinav.sunderrajan/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "lstm_model = fit_lstm(train_scaled, 1, 100, 4)\n",
    "# forecast the entire training dataset to build up state for forecasting\n",
    "train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\n",
    "lstm_model.predict(train_reshaped, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lstm_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fa21f6d4bfa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# make one-step forecast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforecast_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m# invert scaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minvert_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lstm_model' is not defined"
     ]
    }
   ],
   "source": [
    "# walk-forward validation on the test data\n",
    "predictions = list()\n",
    "for i in range(len(test_scaled)):\n",
    "\t# make one-step forecast\n",
    "\tX, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
    "\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "\t# invert scaling\n",
    "\tyhat = invert_scale(scaler, X, yhat)\n",
    "\t# invert differencing\n",
    "\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
    "\t# store forecast\n",
    "\tpredictions.append(yhat)\n",
    "\texpected = raw_values[len(train) + i + 1]\n",
    "\tprint('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
    " \n",
    "# report performance\n",
    "rmse = sqrt(mean_squared_error(raw_values[-12:], predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "# line plot of observed vs predicted\n",
    "pyplot.plot(raw_values[-12:])\n",
    "pyplot.plot(predictions)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
