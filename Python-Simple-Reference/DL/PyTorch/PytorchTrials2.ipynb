{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch tutorials Deep lizard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/home/abhinav.sunderrajan/miniconda3/lib/python3.6/site-packages')\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=torch.Tensor()\n",
    "print(t.dtype,t.device,t.layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__,torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda:0')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torch is strict about data types\n",
    "If an opeartion has to be performed on multiple tensors then they have yo be of the same data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.set_default_dtype(torch.half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=torch.tensor([1,2,3])\n",
    "t2=torch.tensor([1.,2.,3.])\n",
    "print(t1.dtype,t2.dtype)\n",
    "#cannot add\n",
    "t1+t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cannot do operations on tensors residing in CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=torch.tensor([1.,2.,3.])\n",
    "t2=t1.cuda()\n",
    "t1+t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1.dtype)\n",
    "print(t1.half().dtype)\n",
    "print(t1.float().dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2=torch.tanh(t1)\n",
    "print(t2.dtype)\n",
    "print(t2)\n",
    "t2.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create torch tensors from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array([1,2,3])\n",
    "#notice that Tensor change s\n",
    "print(torch.Tensor(data).dtype)\n",
    "print(torch.tensor(data).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.as_tensor(data).dtype)\n",
    "print(torch.from_numpy(data).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create torch tensors without data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.eye(2))\n",
    "print(torch.zeros(2,2))\n",
    "print(torch.randn(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best options for creating tensors\n",
    "Basically we are trying to figure out the different options of creating a torch tensor from the numpy array. Also looking at these different options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array([1,2,3])\n",
    "#below is a factory function\n",
    "t1=torch.tensor(data)\n",
    "#below is the constructor\n",
    "t2=torch.Tensor(data)\n",
    "#below is a factory function\n",
    "t3=torch.as_tensor(data)\n",
    "#below is a factory function\n",
    "t4=torch.from_numpy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default type is float32\n",
    "torch.get_default_dtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1.dtype,t2.dtype,t3.dtype,t4.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float16)\n",
    "t5=torch.Tensor(data)\n",
    "print(t5.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1,t2,t3,t4,t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### as_tensor and from_numpy share data while tensor and Tensor copy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]=0\n",
    "data[1]=0\n",
    "data[2]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1,t2,t3,t4,t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squeeze unsqueeze\n",
    "\n",
    "Squeeze gets rid of 1d axes so basically if a tensor of is of shape (1,9) squeezing turns it into a tensor of shape 9. While squeeze adds that dimension or rank back. Rank of a tensor is the number of dimensios in that tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "k=torch.randn(3,3)\n",
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=k.reshape((1,9)).squeeze()\n",
    "print(r,r.shape)\n",
    "print(r.unsqueeze(dim=0),r.unsqueeze(dim=0).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#faltten is something that you do at the end of a an NN right? That only this is an example of squeeze\n",
    "def flatten(t):\n",
    "    t=t.reshape(1,-1)\n",
    "    t=t.squeeze()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see you get a 1d tensor and you can see how \n",
    "ret=flatten(torch.randn(4,4))\n",
    "print(ret)\n",
    "print(ret.shape,ret.unsqueeze(dim=0).shape,ret.unsqueeze(dim=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is also flatten\n",
    "torch.randn(4,4).reshape(-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "How is it happening below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=torch.tensor([1,2])\n",
    "t2=torch.tensor([[1,1],[1,1]])\n",
    "t1+t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal implict broad casting\n",
    "Internal implict broad casting is done.\n",
    "This is also done with tensors are added (and other operations) with scalars such as t1+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(np.broadcast_to(t1.numpy(),t2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just to show how you can compute channel means of a 3 channel example image\n",
    "Image normailzation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=torch.randint(10,(3,4,4))\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    t1[i]=(t1[i]-t1[i].mean())/t1[i].std()\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argmax sum etc, which are reduction operations\n",
    "\n",
    "The key is to remember that the reduction is element-wise unless we are working on the last dim. The last dim is always just an array of numbers (the scalar components).\n",
    "\n",
    "\n",
    "\n",
    "Suppose we have this tensor:\n",
    "t = \n",
    "[\n",
    " [1,2,3,4],\n",
    " [0,0,0,0]\n",
    "]\n",
    "\n",
    "How do we calculate t.sum(dim=0)?\n",
    "We do it by summing corresponding elements across dim-0.\n",
    "There exist two elements running along dim-0.\n",
    "\n",
    "This one: [1,2,3,4]\n",
    "And this one: [0,0,0,0]\n",
    "\n",
    "Since these are arrays, the sum must be element-wise.\n",
    "\n",
    "This gives us the following:\n",
    "\n",
    "[1+0,2+0,3+0,4+0] = [1,2,3,4]\n",
    "\n",
    "Now, with the argmax(dim=0) operation, it's the same only the operation is different.\n",
    "This time the operation is not sum, but argmax.\n",
    "Argmax gives us the index location of the max value.\n",
    "\n",
    "We have two possible index values, 0 or 1. This is because we are working with two elements. \n",
    "\n",
    "t[0] =  [1,2,3,4]\n",
    "t[1] =  [0,0,0,0]\n",
    "\n",
    "When we do the argmax, we argmax corresponding elements (element-wise) across dim-0.\n",
    "\n",
    "[argmax(1,0), argmax(2,0), argmax(3,0), argmax(4,0)] = [0,0,0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=torch.tensor([[1,2,3,4],[1,2,3,4],[1,2,3,17]])\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete sum, row wise sum and col wise sum\n",
    "t1.sum(),t1.sum(dim=0),t1.sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t1.sum(dim=0)\n",
    "basically done element wise sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1[0]+t1[1]+t1[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t1.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([t1[0].sum(),t1[1].sum(),t1[2].sum()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.argmax(dim=0),t1.max(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.argmax(dim=1),t1.max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "Automatic computation of gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward>) tensor(27., grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How you get 4.5?\n",
    "\n",
    "$$\\frac{dout}{dx_i}=\\frac{1}{4}\\bigg(\\frac{d}{dx_i}(3(x_i+2)^2)\\bigg)$$\n",
    "$$\\frac{dout}{dx_i}=\\frac{3}{2}(x_i+2)=4.5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
