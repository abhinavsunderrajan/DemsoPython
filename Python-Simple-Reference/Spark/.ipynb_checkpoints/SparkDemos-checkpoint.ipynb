{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "Create a notebook in which I have useful pyspark demos based on teh code that I have written over time. Nothing fancy but useful helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T07:29:42.471026Z",
     "start_time": "2019-04-17T07:29:41.995824Z"
    }
   },
   "outputs": [],
   "source": [
    "# pyspark stuff\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#other libraries\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T07:31:39.839380Z",
     "start_time": "2019-04-17T07:31:36.224746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://100.108.215.151:4040'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize spark context\n",
    "session_name = 'geo:{}:oneway_detection'.format(os.environ['JUPYTERHUB_USER'])\n",
    "  \n",
    "#----create a spark session with configurations\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(session_name) \\\n",
    "    .config('spark.cores.max', 32)\\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "#print spark ui weburl\n",
    "spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dummy dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T07:38:18.865449Z",
     "start_time": "2019-04-17T07:38:00.232369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|dma|zip_code|\n",
      "+---+--------+\n",
      "|MIN|   58542|\n",
      "|MIN|   58701|\n",
      "|MAX|   57632|\n",
      "|MAX|   58734|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.createDataFrame([Row(zip_code='58542', dma='MIN'),\n",
    " Row(zip_code='58701', dma='MIN'),\n",
    " Row(zip_code='57632', dma='MAX'),\n",
    " Row(zip_code='58734', dma='MAX')])\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T07:57:56.540655Z",
     "start_time": "2019-04-17T07:57:56.528279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dma: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T07:39:44.027564Z",
     "start_time": "2019-04-17T07:39:44.019637Z"
    }
   },
   "outputs": [],
   "source": [
    "# return multiple values from a pandas UDF\n",
    "schema = StructType([\n",
    "    StructField(\"roadname\", StringType(), False),\n",
    "    StructField(\"roadtype\", StringType(), False),\n",
    "    StructField(\"lanes\", StringType(), False),\n",
    "    StructField(\"is_oneway\", StringType(), False),\n",
    "    StructField(\"is_tunnel\", StringType(), False)\n",
    "])\n",
    "\n",
    "\"\"\"\n",
    "Note that tags is map data type\n",
    "\"\"\"\n",
    "def get_meta_data(tags):\n",
    "    #return tags.get(\"name\")\n",
    "    return Row('roadname', 'roadtype','lanes','is_oneway','is_tunnel')(tags.get(\"name\"), tags.get(\"highway\"),tags.get(\"lanes\"),tags.get(\"oneway\"),tags.get(\"tunnel\"))\n",
    "udf_metadata = udf(f=get_meta_data,returnType= schema)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T07:40:27.556360Z",
     "start_time": "2019-04-17T07:40:27.550793Z"
    }
   },
   "outputs": [],
   "source": [
    "# another udf where I am returning an array of arrays\n",
    "#Get the segment order\n",
    "def get_order(nodeids):\n",
    "    prev=None\n",
    "    segments=[]\n",
    "    for node in nodeids:\n",
    "        if prev==None:\n",
    "            prev=node\n",
    "            continue\n",
    "        else:\n",
    "            segments.append([prev,node])\n",
    "            prev=node\n",
    "    return segments\n",
    "\n",
    "udf_get_order = udf(get_order, ArrayType(ArrayType(LongType()))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T07:46:12.103771Z",
     "start_time": "2019-04-17T07:46:11.842255Z"
    }
   },
   "outputs": [],
   "source": [
    "#always ensure the type of the data that is being returned do not assume\n",
    "#this bloody python and not statically typed language like Java, C++. before returning something\n",
    "#verify else you will burn like I spent 30 minutes debugging this below.\n",
    "\n",
    "from scipy.stats import normaltest\n",
    "def normality_test(x):\n",
    "    return float(normaltest(x)[1])\n",
    "\n",
    "udf_normality_test = udf(f=normality_test,returnType= FloatType()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create external table in hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {table_name} (\n",
    "        prev_way_id long,\n",
    "        way_id long,\n",
    "        turn_count int,\n",
    "        city_id int,\n",
    "        version STRING\n",
    "    )\n",
    "    partitioned by (\n",
    "        date date,\n",
    "        hour int\n",
    "        \n",
    "    )\n",
    "    STORED AS PARQUET\n",
    "    LOCATION 's3a://{bucket_name}/{prefix}/'\n",
    "    '''.format(\n",
    "        table_name=table_name,\n",
    "        bucket_name=bucket_name,\n",
    "        prefix=prefix\n",
    "    )\n",
    "\n",
    "_ = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write dataframe to hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "year=2018\n",
    "month=11\n",
    "city_id=10\n",
    "map_version='v1.1'\n",
    "\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"Etc/GMT\")\n",
    "\n",
    "for day in range(1,7):\n",
    "    date='{}-{:02d}-{:02d}'.format(year,month,day)\n",
    "    for hour in range(0,24):\n",
    "        start_time = time.time()\n",
    "        read_path='s3a://{prefix_name}/{bucket_name}/year={}/month={:02d}/day={:02d}/hour={:02d}'\\\n",
    "        .format(year,month,day,hour)\n",
    "        \n",
    "        path_exists=True\n",
    "        try:\n",
    "            df=spark.read.parquet(read_path)\n",
    "        except Exception as e:\n",
    "            path_exists= False\n",
    "    \n",
    "        if path_exists==False:\n",
    "            print(\"speed data does not exist for \",date, \" for city \",city_id, \" and hour \",hour)\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        df=df.select(\"col1\",\"col2\",\"col3\", F.col(\"qwerty_xyz\").cast(\"long\").alias(\"col4\"),\"time_stamp\")\n",
    "        \n",
    "        #some time stamp related stuff\n",
    "        df=df.withColumn(\"ts\",F.from_unixtime(\"time_stamp\")).withColumn(\"day_of_week\",F.date_format(\"ts\",\"u\"))\n",
    "        \n",
    "        \n",
    "        df=df.orderBy(\"col1\", \"time_stamp\")\n",
    "        col1_counts=df.groupBy(\"col1\").agg(F.count(\"time_stamp\").alias('num_pings'))\n",
    "        df=df.join(col1_counts,(df.BookingCode==col1_counts.BookingCode)).drop(col1_counts.col1)\n",
    "       \n",
    "        \n",
    "        \n",
    "        #windows for transition\n",
    "        wspec=Window.partitionBy('col1').orderBy(\"time_stamp\")\n",
    "        df=df.withColumn(\"prev_col2\",F.lag(df.col2, 1).over(wspec))\n",
    "        df=df.select(['col1','prev_col2','col2']).distinct()\n",
    "        df=df.filter(\"prev_col2 is not NULL\")\n",
    "        df=df.filter(df.col2!=df.prev_col2)\n",
    "        \n",
    "        transition_count=df.groupBy('prev_col2','col2').agg(F.count('col1').alias('turn_count'))\n",
    "        transition_count = transition_count.withColumn(\"city_id\", F.lit(city_id))\n",
    "        transition_count = transition_count.withColumn(\"version\", F.lit(version))\n",
    "        transition_count = transition_count.withColumn(\"date\", F.lit(date))\n",
    "        transition_count = transition_count.withColumn(\"hour\", F.lit(hour))\n",
    "        s3path=\"s3a://{bucket_name}/{prefix}/date={date}/\".format(bucket_name=bucket_name, prefix=prefix,date=date)\n",
    "        \n",
    "        #transition_count.show(10)\n",
    "        #print(transition_count.schema)\n",
    "        transition_count.cache()\n",
    "        transition_count.repartition('date','hour').write.partitionBy('hour').mode('append').format('parquet').save(s3path)\n",
    "        print(\"updating table partitions now at path \",s3path)\n",
    "\n",
    "        hour_path=s3path+\"hour={}/\".format(hour)\n",
    "        sql='''ALTER TABLE {table_name} ADD IF NOT EXISTS PARTITION (date='{date}',hour={hour})\n",
    "        LOCATION '{hour_path}' '''.format(table_name=table_name,date=date,hour=hour,hour_path=hour_path)\n",
    "        _ = spark.sql(sql)\n",
    "        print('finished hour ',hour, 'for date ',date)\n",
    "        transition_count.unpersist()\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
