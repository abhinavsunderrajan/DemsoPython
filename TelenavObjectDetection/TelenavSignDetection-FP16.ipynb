{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Sign Detection using the Telenav DataSet\n",
    "With anchor boxes, focal loss and non max supression.\n",
    "* model_version='3.0' No tuning of the resnet layer\n",
    "* model_version='4.0' is unfreezing of the whole resnet layer and trained has yet yielded significant improvement over 3.0\n",
    "* model 5.0 I am using resnet-50 with fp16 precision, let's see how this works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all the imports right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.\n",
      "Warning:  apex was installed without --cuda_ext. Fused syncbn kernels will be unavailable.  Python fallbacks will be used instead.\n",
      "Warning:  apex was installed without --cuda_ext.  FusedAdam will be unavailable.\n",
      "Warning:  apex was installed without --cuda_ext.  FusedLayerNorm will be unavailable.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches, patheffects\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/home/abhinav.sunderrajan/miniconda3/lib/python3.6/site-packages')\n",
    "#fast ai stuff\n",
    "from fastai.imports import *\n",
    "from fastai.transforms import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.model import *\n",
    "from fastai.dataset import *\n",
    "from fastai.sgdr import *\n",
    "from fastai.plots import *\n",
    "from apex.fp16_utils import FP16_Optimizer\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1 True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__,torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Read the relevant files using pandas and lets take over from there accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='/workspace/datasets/telenav_ai_dataset/'\n",
    "#PATH='/workspace/datasets/mapillary-new/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sign</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SPEED_LIMIT_25_US</th>\n",
       "      <td>2190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPEED_LIMIT_30_US</th>\n",
       "      <td>2342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPEED_LIMIT_35_US</th>\n",
       "      <td>2781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPEED_LIMIT_40_US</th>\n",
       "      <td>2803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPEED_LIMIT_45_US</th>\n",
       "      <td>2426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPEED_LIMIT_55_US</th>\n",
       "      <td>2343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPEED_LIMIT_65_US</th>\n",
       "      <td>934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPEED_LIMIT_70_US</th>\n",
       "      <td>1331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STOP_US</th>\n",
       "      <td>1819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRAFFIC_LIGHTS_SIGN</th>\n",
       "      <td>8726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TURN_RESTRICTION_LEFT_US</th>\n",
       "      <td>4011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TURN_RESTRICTION_RIGHT_US</th>\n",
       "      <td>1526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TURN_RESTRICTION_U_TURN_US</th>\n",
       "      <td>1247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            img_name\n",
       "sign                                \n",
       "SPEED_LIMIT_25_US               2190\n",
       "SPEED_LIMIT_30_US               2342\n",
       "SPEED_LIMIT_35_US               2781\n",
       "SPEED_LIMIT_40_US               2803\n",
       "SPEED_LIMIT_45_US               2426\n",
       "SPEED_LIMIT_55_US               2343\n",
       "SPEED_LIMIT_65_US                934\n",
       "SPEED_LIMIT_70_US               1331\n",
       "STOP_US                         1819\n",
       "TRAFFIC_LIGHTS_SIGN             8726\n",
       "TURN_RESTRICTION_LEFT_US        4011\n",
       "TURN_RESTRICTION_RIGHT_US       1526\n",
       "TURN_RESTRICTION_U_TURN_US      1247"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=pd.read_csv(f'{PATH}telenav_sign_labels.csv')\n",
    "#labels=pd.read_csv(f'{PATH}traffic_sign_labels_mapillary_new.csv')\n",
    "bb_boxes=pd.read_csv(f'{PATH}telenav_sign_bbox.csv')\n",
    "#bb_boxes=pd.read_csv(f'{PATH}traffic_sign_bbox_mapillary_new.csv')\n",
    "labels.groupby(['sign']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>471860_50ba6_19.jpg</td>\n",
       "      <td>TRAFFIC_LIGHTS_SIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>504879_56d28_36.jpg</td>\n",
       "      <td>SPEED_LIMIT_40_US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>331295_75541_5920cac878fe4.jpg</td>\n",
       "      <td>TRAFFIC_LIGHTS_SIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>475460_2d71c_42.jpg</td>\n",
       "      <td>TRAFFIC_LIGHTS_SIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1289_16ada_56fed679c7e15.jpeg</td>\n",
       "      <td>TURN_RESTRICTION_LEFT_US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         img_name                      sign\n",
       "0             471860_50ba6_19.jpg       TRAFFIC_LIGHTS_SIGN\n",
       "1             504879_56d28_36.jpg         SPEED_LIMIT_40_US\n",
       "2  331295_75541_5920cac878fe4.jpg       TRAFFIC_LIGHTS_SIGN\n",
       "3             475460_2d71c_42.jpg       TRAFFIC_LIGHTS_SIGN\n",
       "4   1289_16ada_56fed679c7e15.jpeg  TURN_RESTRICTION_LEFT_US"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and save multiple bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27675, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>bbox</th>\n",
       "      <th>count_bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09cfb_56df5295081ae.jpg</td>\n",
       "      <td>962 576 1141 663</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10014_e0a45_578f86aae21c0.jpg</td>\n",
       "      <td>1646 1512 1890 1629 1480 3578 1688 3695</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10051_adb58_5790460a2a53b.jpg</td>\n",
       "      <td>1305 2568 1421 2703</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10053_2ddf1_5790486a1b320.jpg</td>\n",
       "      <td>1307 369 1440 459 1141 2486 1313 2665</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10053_47b84_579048491f4ac.jpg</td>\n",
       "      <td>1076 3025 1224 3182</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        img_name                                     bbox  \\\n",
       "0        09cfb_56df5295081ae.jpg                         962 576 1141 663   \n",
       "1  10014_e0a45_578f86aae21c0.jpg  1646 1512 1890 1629 1480 3578 1688 3695   \n",
       "2  10051_adb58_5790460a2a53b.jpg                      1305 2568 1421 2703   \n",
       "3  10053_2ddf1_5790486a1b320.jpg    1307 369 1440 459 1141 2486 1313 2665   \n",
       "4  10053_47b84_579048491f4ac.jpg                      1076 3025 1224 3182   \n",
       "\n",
       "   count_bbox  \n",
       "0           1  \n",
       "1           2  \n",
       "2           1  \n",
       "3           2  \n",
       "4           1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MBB_CSV = PATH+'traffic_signs_mbb.csv'\n",
    "def get_box(group):\n",
    "    seperator = ' '\n",
    "    return pd.Series({'bbox':seperator.join(group.bbox),'count_bbox':group.shape[0]})\n",
    "df_bbox=bb_boxes.groupby(['img_name']).apply(get_box)\n",
    "df_bbox.reset_index(inplace=True)\n",
    "df_bbox.to_csv(MBB_CSV, index=False)\n",
    "print(df_bbox.shape)\n",
    "df_bbox.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and save multiclass\n",
    "Watch out with apply functionsince it calls the first element twice. See link https://stackoverflow.com/questions/21390035/python-pandas-groupby-object-apply-method-duplicates-first-group. Thats why I am dropping the first element in mc list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27675, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>count_signs</th>\n",
       "      <th>sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09cfb_56df5295081ae.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>TRAFFIC_LIGHTS_SIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10014_e0a45_578f86aae21c0.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>TRAFFIC_LIGHTS_SIGN TRAFFIC_LIGHTS_SIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10051_adb58_5790460a2a53b.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>TURN_RESTRICTION_LEFT_US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10053_2ddf1_5790486a1b320.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>TRAFFIC_LIGHTS_SIGN TURN_RESTRICTION_RIGHT_US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10053_47b84_579048491f4ac.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>TURN_RESTRICTION_RIGHT_US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        img_name  count_signs  \\\n",
       "0        09cfb_56df5295081ae.jpg            1   \n",
       "1  10014_e0a45_578f86aae21c0.jpg            2   \n",
       "2  10051_adb58_5790460a2a53b.jpg            1   \n",
       "3  10053_2ddf1_5790486a1b320.jpg            2   \n",
       "4  10053_47b84_579048491f4ac.jpg            1   \n",
       "\n",
       "                                            sign  \n",
       "0                            TRAFFIC_LIGHTS_SIGN  \n",
       "1        TRAFFIC_LIGHTS_SIGN TRAFFIC_LIGHTS_SIGN  \n",
       "2                       TURN_RESTRICTION_LEFT_US  \n",
       "3  TRAFFIC_LIGHTS_SIGN TURN_RESTRICTION_RIGHT_US  \n",
       "4                      TURN_RESTRICTION_RIGHT_US  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MC_CSV = PATH+'traffic_signs_multi_class.csv'\n",
    "mc=[]\n",
    "def get_signs(group):\n",
    "    seperator = ' '\n",
    "    mc.append(list(group.sign))\n",
    "    return pd.Series({'sign':seperator.join(group.sign),'count_signs':group.shape[0]})\n",
    "\n",
    "df_labels=labels.groupby(['img_name']).apply(get_signs)\n",
    "df_labels.reset_index(inplace=True)\n",
    "df_labels.to_csv(MC_CSV, index=False)\n",
    "mc=mc[1:]\n",
    "print(df_labels.shape)\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "img_name                          25327_987eb_580554719bfe0.jpg\n",
       "bbox          1884 4540 2003 4662 1812 4256 2135 4447 841 24...\n",
       "count_bbox                                                    7\n",
       "Name: 3897, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bbox.loc[df_bbox['count_bbox'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataloader for image labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2cats=list(labels.sign.unique())\n",
    "cats2id = {v:k for k,v in enumerate(id2cats)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_fns={(k+1):v for k,v in enumerate(df_labels.img_name.unique())}\n",
    "mcs = np.array([np.array([cats2id[p] for p in o]) for o in mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get validation set\n",
    "20 percent of the dataset by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idxs = get_cv_idxs(len(trn_fns))\n",
    "((val_mcs,trn_mcs),) = split_by_idx(val_idxs, mcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_model=resnext50\n",
    "#the number of filters is 512 for resnet34 and 2048 for resnet50\n",
    "n_filters=2048\n",
    "sz=512\n",
    "bs=32\n",
    "jaccard_overlap=0.4\n",
    "model_version='5.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for bounding box\n",
    "aug_tfms = [#RandomRotate(10, tfm_y=TfmType.COORD),\n",
    "            RandomLighting(0.05, 0.05, tfm_y=TfmType.COORD),\n",
    "            RandomFlip(tfm_y=TfmType.COORD)]\n",
    "tfms = tfms_from_model(f_model, sz, crop_type=CropType.NO, tfm_y=TfmType.COORD, aug_tfms=aug_tfms)\n",
    "#md = ImageClassifierData.from_csv(PATH, 'train_data', MBB_CSV,bs=bs, tfms=tfms, continuous=True)\n",
    "md = ImageClassifierData.from_csv(PATH, 'train_data', MBB_CSV,bs=bs, tfms=tfms, continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatLblDataset(Dataset):\n",
    "    def __init__(self, ds, y2):\n",
    "        self.ds,self.y2 = ds,y2\n",
    "        self.sz = ds.sz\n",
    "    def __len__(self): return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        x,y = self.ds[i]\n",
    "        return (x, (y,self.y2[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a feel of the batch sizes and shape of the input tensors and also plot them ground truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds2 = ConcatLblDataset(md.trn_ds, trn_mcs)\n",
    "val_ds2 = ConcatLblDataset(md.val_ds, val_mcs)\n",
    "md.trn_dl.dataset = trn_ds2\n",
    "md.val_dl.dataset = val_ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=to_np(next(iter(md.val_dl)))\n",
    "x=md.val_ds.ds.denorm(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[0].shape,y[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def get_cmap(N):\n",
    "    color_norm  = mcolors.Normalize(vmin=0, vmax=N-1)\n",
    "    return cmx.ScalarMappable(norm=color_norm, cmap='Set3').to_rgba\n",
    "\n",
    "num_colr = 12\n",
    "cmap = get_cmap(num_colr)\n",
    "colr_list = [cmap(float(x)) for x in range(num_colr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_to_hw(bb):\n",
    "    return [bb[1],bb[0],bb[3]-bb[1], bb[2]-bb[0]]\n",
    "\n",
    "def show_img(im, figsize=None, axis=None):\n",
    "    if not axis:\n",
    "        fig,axis = plt.subplots(figsize=figsize)\n",
    "    axis.imshow(im)\n",
    "    axis.get_xaxis().set_visible(False)\n",
    "    axis.get_yaxis().set_visible(False)\n",
    "    return axis\n",
    "\n",
    "def draw_outline(obj,lw):\n",
    "    obj.set_path_effects([patheffects.Stroke(linewidth=lw, foreground='black'), patheffects.Normal()])\n",
    "    \n",
    "def draw_rect(axis, box, color='white'):\n",
    "    patch = axis.add_patch(patches.Rectangle(box[:2],box[-2],box[-1],fill=False,edgecolor=color,lw=2))\n",
    "    draw_outline(patch,4)\n",
    "    \n",
    "def draw_text(axis,xy,text,text_size=14, color='white'):\n",
    "    patch = axis.text(*xy, text, verticalalignment='top', color=color, fontsize=text_size, weight='bold')\n",
    "    draw_outline(patch,1)\n",
    "    \n",
    "def show_img_all(id_img):\n",
    "    img = open_image(IMG_PATH/imgs_fn[id_img])\n",
    "    axis = show_img(img, figsize=(16,8))\n",
    "    for bbox, id_cat in trn_anno[id_img]:\n",
    "        new_box = bb_to_hw(bbox)\n",
    "        draw_rect(axis, new_box)\n",
    "        draw_text(axis, new_box[:2], cats[id_cat])\n",
    "        \n",
    "def show_ground_truth(ax, im, bbox, clas = None, prs = None, tresh = 0.3):\n",
    "    bb = [bb_to_hw(o) for o in bbox.reshape(-1,4)]\n",
    "    if clas is None: clas = [None] * len(bb)\n",
    "    if prs is None: prs = [None] * len(bb)\n",
    "    ax = show_img(im,axis=ax)\n",
    "    for i, (b,c,pr) in enumerate(zip(bb,clas,prs)):\n",
    "        if b[2] > 0 and (pr is None or pr > tresh):#Show the bow only if there is something to show\n",
    "            draw_rect(ax, b, colr_list[i%num_colr])\n",
    "            txt = f'{i}: '\n",
    "            if c is not None: txt += ('bg' if c == len(id2cats) else id2cats[c])\n",
    "            if pr is not None: txt += f'{pr:.2f}'\n",
    "            draw_text(ax,b[:2],txt,color=colr_list[i%num_colr])\n",
    "        \n",
    "def torch_gt(ax, ima, bbox, clas, prs=None, thresh=0.4):\n",
    "    return show_ground_truth(ax, ima, to_np((bbox*sz).long()),\n",
    "         to_np(clas), to_np(prs) if prs is not None else None, thresh)\n",
    "\n",
    "def np_gt(ax, ima, bbox, clas, prs=None, thresh=0.4):\n",
    "    return show_ground_truth(ax, ima, (bbox*sz).astype(np.uint8),\n",
    "         clas, prs if prs is not None else None, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(20, 20))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    show_ground_truth(ax, x[i+2], y[0][i+2], y[1][i+2])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hw2corners(ctr, hw): \n",
    "    return torch.cat([ctr-hw/2,ctr+hw/2], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anc_grids = [8,4,1]\n",
    "#anc_grids = [4]\n",
    "anc_zooms = [0.25,0.75,1.0,1.5]\n",
    "# anc_zooms = [1.]\n",
    "anc_ratios = [(1.,1.), (1.,0.5), (0.5,1.)]\n",
    "# anc_ratios = [(1.,1.)]\n",
    "anchor_scales = [(anz*i,anz*j) for anz in anc_zooms for (i,j) in anc_ratios]\n",
    "k = len(anchor_scales)\n",
    "anc_offsets = [1/(o*2) for o in anc_grids]\n",
    "anc_x = np.concatenate([np.tile(np.linspace(ao, 1-ao, ag), ag)\n",
    "                        for ao,ag in zip(anc_offsets,anc_grids)])\n",
    "anc_y = np.concatenate([np.repeat(np.linspace(ao, 1-ao, ag), ag)\n",
    "                        for ao,ag in zip(anc_offsets,anc_grids)])\n",
    "anc_ctrs = np.repeat(np.stack([anc_x,anc_y], axis=1), k, axis=0)\n",
    "anc_sizes  =   np.concatenate([np.array([[o/ag,p/ag] for i in range(ag*ag) for o,p in anchor_scales])\n",
    "               for ag in anc_grids])\n",
    "grid_sizes = V(np.concatenate([np.array([ 1/ag       for i in range(ag*ag) for o,p in anchor_scales])\n",
    "               for ag in anc_grids]), requires_grad=False).half().unsqueeze(1)\n",
    "\n",
    "#grid_sizes = V(np.concatenate([np.array([ 1/ag       for i in range(ag*ag) for o,p in anchor_scales])\n",
    "#               for ag in anc_grids]), requires_grad=False).unsqueeze(1)\n",
    "\n",
    "anchors = V(np.concatenate([anc_ctrs, anc_sizes], axis=1), requires_grad=False).half()\n",
    "#anchors = V(np.concatenate([anc_ctrs, anc_sizes], axis=1), requires_grad=False)\n",
    "anchor_cnr = hw2corners(anchors[:,:2], anchors[:,2:]).half()\n",
    "#anchor_cnr = hw2corners(anchors[:,:2], anchors[:,2:])\n",
    "\n",
    "#k is the number of zooms multiplied by the number of aspect ratios\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Pyramid Networks (FPN)\n",
    "Check out this nice blog post https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StdConv(nn.Module):\n",
    "    def __init__(self, n_in,n_out,stride=2,padding=1,dp = 0.1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_in,n_out,3,stride=stride,padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(n_out)\n",
    "        self.dropout = nn.Dropout(dp,inplace=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.dropout(self.bn(F.relu(self.conv(x))))\n",
    "    \n",
    "def flatten_conv(x,k):\n",
    "    bs,nf,gx,gy = x.size()\n",
    "    x = x.permute(0,2,3,1).contiguous()\n",
    "    return x.view(bs,-1,nf//k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is out conv?\n",
    "Outconv essentially returns two convolutional layers one of size number_of_cateories (*k) plus one for background to sepcialize in object categorization and the other of size 4 (*k) for the bounding box detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutConv(nn.Module):\n",
    "    def __init__(self, k, n_in, bias):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.oconv1 = nn.Conv2d(n_in, (len(id2cats)+1) * k, 3, padding=1)\n",
    "        self.oconv2 = nn.Conv2d(n_in, 4 * k, 3, padding = 1)\n",
    "        self.oconv1.bias.data.zero_().add_(bias)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return [flatten_conv(self.oconv1(x), self.k),\n",
    "                flatten_conv(self.oconv2(x), self.k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop=0.4\n",
    "\n",
    "class SSD_MultiHead(nn.Module):\n",
    "    def __init__(self, k, bias):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(drop,inplace=True)\n",
    "        self.sconv1 = StdConv(n_filters,256, dp=drop)\n",
    "        self.sconv2 = StdConv(256,256, dp=drop)\n",
    "        self.sconv3 = StdConv(256,256,padding=0,dp=drop)\n",
    "        self.out1 = OutConv(k, 256, bias)\n",
    "        self.out2 = OutConv(k, 256, bias)\n",
    "        self.out3 = OutConv(k, 256, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop(F.relu(x))\n",
    "        x = self.sconv1(x)\n",
    "        o1c,o1l = self.out1(x)\n",
    "        x = self.sconv2(x)\n",
    "        o2c,o2l = self.out2(x)\n",
    "        x = self.sconv3(x)\n",
    "        o3c,o3l = self.out3(x)\n",
    "        return [torch.cat([o1c,o2c,o3c], dim=1),\n",
    "                torch.cat([o1l,o2l,o3l], dim=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "Focal loss from the retina net paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_hot_embedding(labels, num_classes):\n",
    "    return torch.eye(num_classes)[labels.data.cpu()]\n",
    "\n",
    "class BCE_Loss(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, preds, targets):\n",
    "        t = one_hot_embedding(targets, self.num_classes+1)\n",
    "        #bg class is predicted when none of the others go out.\n",
    "        t = V(t[:,:-1].contiguous()).half() \n",
    "        #t = V(t[:,:-1].contiguous()) \n",
    "        x = preds[:,:-1]\n",
    "        w = self.get_weight(x,t)# for the last part\n",
    "        return F.binary_cross_entropy_with_logits(x, t, w, size_average=False) / self.num_classes\n",
    "    \n",
    "    def get_weight(self,x,t):\n",
    "        return None\n",
    "\n",
    "class FocalLoss(BCE_Loss):\n",
    "    def get_weight(self,x,t):\n",
    "        alpha,gamma = 0.25,2.\n",
    "        p = x.sigmoid()\n",
    "        pt = p*t + (1-p)*(1-t)\n",
    "        w = alpha*t + (1-alpha)*(1-t)\n",
    "        return w * (1-pt).pow(gamma)\n",
    "\n",
    "loss_f = FocalLoss(len(id2cats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IOU and jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(box_a,box_b):\n",
    "    min_xy = torch.max(box_a[:,None,:2],box_b[None,:,:2])\n",
    "    max_xy = torch.min(box_a[:,None,2:],box_b[None,:,2:])\n",
    "    inter = torch.clamp(max_xy-min_xy,min=0)\n",
    "    return inter[:,:,0] * inter[:,:,1]\n",
    "\n",
    "def get_size(box):\n",
    "    return (box[:,2]-box[:,0]) * (box[:,3] - box[:,1])\n",
    "\n",
    "def jaccard(box_a,box_b):\n",
    "    box_a=box_a.half()\n",
    "    box_b=box_b.half()\n",
    "    inter = intersection(box_a,box_b)\n",
    "    union = get_size(box_a).unsqueeze(1) + get_size(box_b).unsqueeze(0) - inter\n",
    "    return inter/union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes the zero padding in the target bbox/class\n",
    "def get_y(bbox,clas):\n",
    "    bbox = bbox.view(-1,4)/sz\n",
    "    bb_keep = ((bbox[:,2] - bbox[:,0])>0.).nonzero()[:,0]\n",
    "    #print(bbox)\n",
    "    #print(clas)\n",
    "    #print('--------------------------------------')\n",
    "    return bbox[bb_keep], clas[bb_keep]\n",
    "\n",
    "\n",
    "def actn_to_bb(actn, anchors):\n",
    "    \"\"\"\n",
    "    interpreting activations and converting them to bouding boxes. \n",
    "    The method is the short form for activations to bounding boxes.\n",
    "    \"\"\"\n",
    "    actn_bbs = torch.tanh(actn)\n",
    "    actn_ctrs = (actn_bbs[:,:2] * grid_sizes/2) + anchors[:,:2]\n",
    "    actn_hw = (1 + actn_bbs[:,2:]/2) * anchors[:,2:]\n",
    "    return hw2corners(actn_ctrs,actn_hw)\n",
    "\n",
    "def map_to_ground_truth(overlaps, print_it=False):\n",
    "    prior_overlap, prior_idx = overlaps.max(1)\n",
    "    if print_it: print(prior_overlap)\n",
    "#     pdb.set_trace()\n",
    "    gt_overlap, gt_idx = overlaps.max(0)\n",
    "    gt_overlap[prior_idx] = 1.99\n",
    "    for i,o in enumerate(prior_idx): gt_idx[o] = i\n",
    "    return gt_overlap,gt_idx\n",
    "\n",
    "def ssd_1_loss(b_c,b_bb,bbox,clas,print_it=False, use_ab=True):\n",
    "    bbox,clas = get_y(bbox,clas)\n",
    "    bbox=bbox.half()\n",
    "    a_ic = actn_to_bb(b_bb, anchors)\n",
    "    a_ic=a_ic.half()\n",
    "    #'Yes' if fruit == 'Apple' else 'No'\n",
    "    overlaps = jaccard(bbox.data, (anchor_cnr if use_ab else a_ic).data)\n",
    "    gt_overlap,gt_idx = map_to_ground_truth(overlaps,print_it)\n",
    "    gt_clas = clas[gt_idx]\n",
    "    pos = gt_overlap > jaccard_overlap\n",
    "    pos_idx = torch.nonzero(pos)[:,0]\n",
    "    gt_clas[1-pos] = len(id2cats)\n",
    "    gt_bbox = bbox[gt_idx]\n",
    "    gt_bbox=gt_bbox.half()\n",
    "    loc_loss = ((a_ic[pos_idx] - gt_bbox[pos_idx]).abs()).mean()\n",
    "    clas_loss  = loss_f(b_c, gt_clas)\n",
    "    return loc_loss, clas_loss\n",
    "\n",
    "def ssd_loss(pred,targ,print_it=False):\n",
    "    lcs,lls = 0.,0.\n",
    "    for b_c,b_bb,bbox,clas in zip(*pred,*targ):\n",
    "        loc_loss,clas_loss = ssd_1_loss(b_c,b_bb,bbox,clas,print_it)\n",
    "        lls += loc_loss\n",
    "        lcs += clas_loss\n",
    "    if print_it: print(f'loc: {lls.data[0]}, clas: {lcs.data[0]}')\n",
    "    return lls+lcs\n",
    "\n",
    "def ssd_loss2(pred,targ):\n",
    "    lcs,lls = 0.,0.\n",
    "    for b_c,b_bb,bbox,clas in zip(*pred,*targ):\n",
    "        loc_loss,clas_loss = ssd_1_loss(b_c,b_bb,bbox,clas,use_ab=False)\n",
    "        lls += loc_loss\n",
    "        lcs += clas_loss\n",
    "    return lls+lcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fp16v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class tofp16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(tofp16, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.half()\n",
    "\n",
    "\n",
    "def copy_in_params(net, params):\n",
    "    net_params = list(net.parameters())\n",
    "    for i in range(len(params)):\n",
    "        net_params[i].data.copy_(params[i].data)\n",
    "\n",
    "\n",
    "def set_grad(params, params_with_grad):\n",
    "\n",
    "    for param, param_w_grad in zip(params, params_with_grad):\n",
    "        if param.grad is None:\n",
    "            param.grad = torch.nn.Parameter(param.data.new().resize_(*param.data.size()))\n",
    "        param.grad.data.copy_(param_w_grad.grad.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BN_convert_float(module):\n",
    "    '''\n",
    "    BatchNorm layers to have parameters in single precision.\n",
    "    Find all layers and convert them back to float. This can't\n",
    "    be done with built in .apply as that function will apply\n",
    "    fn to all modules, parameters, and buffers. Thus we wouldn't\n",
    "    be able to guard the float conversion based on the module type.\n",
    "    '''\n",
    "    if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n",
    "        module.float()\n",
    "    for child in module.children():\n",
    "        BN_convert_float(child)\n",
    "    return module\n",
    "\n",
    "\n",
    "def network_to_half(network):\n",
    "    return nn.Sequential(tofp16(), BN_convert_float(network.half()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_reg4 = SSD_MultiHead(k, -4.)\n",
    "models = ConvnetBuilder(f_model, 0, 0, 0, custom_head=head_reg4)\n",
    "\n",
    "models.model = network_to_half(models.model)\n",
    "learn = ConvLearner(md, models)\n",
    "learn.opt_fn=optim.Adam\n",
    "learn.crit = ssd_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "Run this block only when you train a model. If you have a pretrained model skip and move to the subsequent steps.\n",
    "##### Note that model traning takes 10 minutes per epoch on an average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "lrs = np.array([lr/100,lr/10,lr])\n",
    "learn.lr_find(lrs/1000,1.)\n",
    "learn.sched.plot(n_skip_end=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "lrs = np.array([lr/50,lr/5,lr])\n",
    "learn.fit(lrs, 1, cycle_len=30, use_clr=(32,5))\n",
    "learn.sched.plot_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use differential learning rates\n",
    "Now start tuning the resnet block. I set very low learning rates for the first few layers which progressively increases as we start approaching the custom head  i.e. the Feature Pyramid Networks (FPN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unable unfreeze resnet50 due to out of memory exceptions\n",
    "learn.unfreeze()\n",
    "#learn.freeze_to(-2)\n",
    "learn.fit(lrs/30, 1, cycle_len=28, use_clr=(32,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'mapillary-{sz}-{bs}-atleast-900-{model_version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'saved model mapillary-{sz}-{bs}-atleast-900-{model_version}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMS\n",
    "Nonmax supression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(boxes, scores, overlap=0.5, top_k=100):\n",
    "    keep = scores.new(scores.size(0)).zero_().long()\n",
    "    if boxes.numel() == 0: return keep\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    area = torch.mul(x2 - x1, y2 - y1)\n",
    "    v, idx = scores.sort(0)  # sort in ascending order\n",
    "    idx = idx[-top_k:]  # indices of the top-k largest vals\n",
    "    xx1 = boxes.new()\n",
    "    yy1 = boxes.new()\n",
    "    xx2 = boxes.new()\n",
    "    yy2 = boxes.new()\n",
    "    w = boxes.new()\n",
    "    h = boxes.new()\n",
    "\n",
    "    count = 0\n",
    "    while idx.numel() > 0:\n",
    "        i = idx[-1]  # index of current largest val\n",
    "        keep[count] = i\n",
    "        count += 1\n",
    "        if idx.size(0) == 1: break\n",
    "        idx = idx[:-1]  # remove kept element from view\n",
    "        # load bboxes of next highest vals\n",
    "        torch.index_select(x1, 0, idx, out=xx1)\n",
    "        torch.index_select(y1, 0, idx, out=yy1)\n",
    "        torch.index_select(x2, 0, idx, out=xx2)\n",
    "        torch.index_select(y2, 0, idx, out=yy2)\n",
    "        # store element-wise max with next highest score\n",
    "        xx1 = torch.clamp(xx1, min=x1[i])\n",
    "        yy1 = torch.clamp(yy1, min=y1[i])\n",
    "        xx2 = torch.clamp(xx2, max=x2[i])\n",
    "        yy2 = torch.clamp(yy2, max=y2[i])\n",
    "        w.resize_as_(xx2)\n",
    "        h.resize_as_(yy2)\n",
    "        w = xx2 - xx1\n",
    "        h = yy2 - yy1\n",
    "        # check sizes of xx1 and xx2.. after each iteration\n",
    "        w = torch.clamp(w, min=0.0)\n",
    "        h = torch.clamp(h, min=0.0)\n",
    "        inter = w*h\n",
    "        # IoU = i / (area(a) + area(b) - i)\n",
    "        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n",
    "        union = (rem_areas - inter) + area[i]\n",
    "        IoU = inter/union  # store result in iou\n",
    "        # keep only elements with an IoU <= overlap\n",
    "        idx = idx[IoU.le(overlap)]\n",
    "    return keep, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.load(f'telenav-{sz}-{bs}-atleast-900-{model_version}')\n",
    "learn.load(f'mapillary-{sz}-{bs}-atleast-900-{model_version}')\n",
    "learn.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on a minibatch in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num=2\n",
    "i=0\n",
    "x=None\n",
    "y=None\n",
    "for data in md.val_dl:\n",
    "    if i==batch_num:\n",
    "        x,y = data\n",
    "        x,y = V(x),V(y)\n",
    "        break\n",
    "    i=i+1    \n",
    "pred = learn.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_overlap=0.4\n",
    "def get1preds(b_clas,b_bb,bbox,clas,thresh=0.25):\n",
    "    bbox,clas = get_y(bbox, clas)\n",
    "    a_ic = actn_to_bb(b_bb, anchors)\n",
    "    clas_pr, clas_ids = b_clas.max(1)\n",
    "    conf_scores = b_clas.sigmoid().t().data\n",
    "    out1,out2,cc = [],[],[]\n",
    "    for cl in range(conf_scores.size(0)-1):\n",
    "        cl_mask = conf_scores[cl] > thresh\n",
    "        if cl_mask.sum() == 0: continue\n",
    "        scores = conf_scores[cl][cl_mask]\n",
    "        l_mask = cl_mask.unsqueeze(1).expand_as(a_ic)\n",
    "        boxes = a_ic[l_mask].view(-1, 4)\n",
    "        ids, count = nms(boxes.data, scores, jaccard_overlap, 1)\n",
    "        ids = ids[:count]\n",
    "        out1.append(scores[ids])\n",
    "        out2.append(boxes.data[ids])\n",
    "        cc.append([cl]*count)\n",
    "    cc = T(np.concatenate(cc)) if cc != [] else None\n",
    "    out1 = torch.cat(out1) if out1 != [] else None\n",
    "    out2 = torch.cat(out2) if out2 != [] else None\n",
    "    return out1,out2,cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(idx, thresh=0.25, ax=None):\n",
    "    if ax is None: \n",
    "        fig, ax = plt.subplots(figsize=(6,6))\n",
    "    ax.title.set_text(\"Model results\")\n",
    "        \n",
    "    ima=md.val_ds.ds.denorm(x)[idx]\n",
    "    out1,out2,cc = get1preds(pred[0][idx],pred[1][idx],y[0][idx],y[1][idx],thresh)\n",
    "    torch_gt(ax, ima, out2, cc, out1, thresh)\n",
    "    \n",
    "def show_gt(idx, ax=None):\n",
    "    if ax is None: \n",
    "        fig, ax = plt.subplots(figsize=(6,6))\n",
    "    ax.title.set_text(\"Ground truth\")\n",
    "    ima = md.val_ds.ds.denorm(x)[idx]\n",
    "    show_ground_truth(ax,ima,to_np(y[0][idx]),to_np(y[1][idx]))\n",
    "    \n",
    "def compare(idx,thresh=0.5):\n",
    "    fig, axs = plt.subplots(2,1,figsize=(6,12))\n",
    "    show_results(idx,thresh,ax=axs[0])\n",
    "    show_gt(idx,ax=axs[1])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=19\n",
    "compare(idx,thresh=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the anchor boxes\n",
    "if you want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b_clas,b_bb = pred\n",
    "#idx=7\n",
    "#b_clasi = b_clas[idx]\n",
    "#b_bboxi = b_bb[idx]\n",
    "#ima=md.val_ds.ds.denorm(to_np(x))[idx]\n",
    "#bbox,clas = get_y(y[0][idx], y[1][idx])\n",
    "\n",
    "#fig, ax = plt.subplots(figsize=(17,17))\n",
    "#torch_gt(ax, ima, anchor_cnr, b_clasi.max(1)[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean average precision\n",
    "Calculate the mean average precision of your classifier. See the link below for a wonderful tutorial on it.\n",
    "\n",
    "https://medium.com/@timothycarlen/understanding-the-map-evaluation-metric-for-object-detection-a07fe6962cf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(L):\n",
    "    result = collections.defaultdict(int)\n",
    "    if L is not None:\n",
    "        for x in L:\n",
    "            result[x] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count([1,14,1,14,1,2,3,5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_overlap=0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "def multiTPFPFN():\n",
    "    n = 40\n",
    "    threshes = np.linspace(0.001, 0.95, n, endpoint=True)\n",
    "    tps,fps,fns = np.zeros((n,len(id2cats))),np.zeros((n,len(id2cats))),np.zeros((n,len(id2cats)))\n",
    "    prog = FloatProgress(min=0,max=len(md.val_dl))\n",
    "    display(prog)\n",
    "    for data in md.val_dl:\n",
    "        x,y = data\n",
    "        x,y = V(x),V(y)\n",
    "        pred = learn.model(x)\n",
    "        for idx in range(x.size(0)):\n",
    "            bbox,clas = get_y(y[0][idx],y[1][idx])#unpad the target\n",
    "            p_scrs,p_box,p_cls = get1preds(pred[0][idx],pred[1][idx],y[0][idx],y[1][idx],threshes[0])\n",
    "            overlaps = to_np(jaccard(p_box,bbox.data))\n",
    "            overlaps = np.where(overlaps > min_overlap, overlaps, 0)\n",
    "            clas, np_scrs, np_cls = to_np(clas.data),to_np(p_scrs), to_np(p_cls)\n",
    "            for k in range(threshes.shape[0]):\n",
    "                new_tp = collections.defaultdict(int)\n",
    "                for cls in list(set(clas)):\n",
    "                    msk_clas = np.bitwise_and((clas == cls)[None,:],(np_cls == cls)[:,None])\n",
    "                    ov_clas = np.where(msk_clas,overlaps,0.)\n",
    "                    mx_idx = np.argmax(ov_clas,axis=1)\n",
    "                    for i in range(0,len(clas)):\n",
    "                        if (clas[i] == cls):\n",
    "                            keep = np.bitwise_and(np.max(ov_clas,axis=1) > 0.,mx_idx==i)\n",
    "                            keep = np.bitwise_and(keep,np_scrs > threshes[k])\n",
    "                            if keep.sum() > 0:\n",
    "                                new_tp[cls] += 1\n",
    "                count_pred = count(np_cls[np_scrs > threshes[k]])\n",
    "                count_gt = count(clas)\n",
    "                for c in range(len(id2cats)):\n",
    "                    tps[k,c] += new_tp[c]\n",
    "                    fps[k,c] += count_pred[c] - new_tp[c]\n",
    "                    fns[k,c] += count_gt[c] - new_tp[c]\n",
    "        prog.value += 1 \n",
    "    return tps, fps, fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tps, fps, fns = multiTPFPFN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "cmap = plt.get_cmap('jet_r')\n",
    "N=len(id2cats)\n",
    "def plot_prec_recall(clas):\n",
    "    color = cmap(float(i)/N)\n",
    "    prec = np.where(tps[:,clas] + fps[:,clas] != 0, tps[:,clas]/(tps[:,clas] + fps[:,clas]), 1)\n",
    "    recal = np.where(tps[:,clas] + fns[:,clas] != 0, tps[:,clas]/(tps[:,clas] + fns[:,clas]), 1)\n",
    "    plt.plot(recal,prec,label=id2cats[clas],c=color,linewidth=3.0)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(id2cats)):\n",
    "    plot_prec_recall(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_prec(clas):\n",
    "    precisions = np.where(tps[:,clas] + fps[:,clas] != 0, tps[:,clas]/(tps[:,clas] + fps[:,clas]), 1)\n",
    "    recalls = np.where(tps[:,clas] + fns[:,clas] != 0, tps[:,clas]/(tps[:,clas] + fns[:,clas]), 1)\n",
    "    prec_at_rec = []\n",
    "    for recall_level in np.linspace(0.0, 1.0, 11):\n",
    "        try:\n",
    "            args = np.argwhere(recalls >= recall_level).flatten()\n",
    "            prec = max(precisions[args])\n",
    "        except ValueError:\n",
    "            prec = 0.0\n",
    "        prec_at_rec.append(prec)\n",
    "    return np.array(prec_at_rec).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mAP():\n",
    "    S = 0\n",
    "    for i in range(len(id2cats)):\n",
    "        S += avg_prec(i)\n",
    "    return S/len(id2cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report mAP\n",
    "map much better now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??learn.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
